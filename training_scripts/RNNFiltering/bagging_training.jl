using Flux
using Flux: truncate!, reset!, batchseq, @epochs, params
using POMDPs
using POMDPToolbox
using AutomotiveDrivingModels
using AutomotivePOMDPs
using AutomotiveSensors
using PedCar
using UniversalTensorBoard
using ArgParse
using JLD2
using FileIO
using ProgressMeter
include("RNNFiltering.jl")
using RNNFiltering
using BSON
using BSON: @save
s = ArgParseSettings()
@add_arg_table s begin
    "--seed"
        help = "an integer for the MersenneTwister"
        arg_type = Int64
        default = 1
    "--resume"
        help = "resume training of an existing model"
        arg_type = Int64
        default = -1
end
parsed_args = parse_args(ARGS, s)

## RNG SEED 
seed = parsed_args["seed"]
rng = MersenneTwister(seed)
srand(rng)

# # init continuous state mdp 
mdp = PedCarMDP(pos_res=2.0, vel_res=2., ped_birth=0.7, car_birth=0.7)
pomdp = UrbanPOMDP(env=mdp.env,
                    sensor = GaussianSensor(false_positive_rate=0.05, 
                                            pos_noise = LinearNoise(min_noise=0.5, increase_rate=0.05), 
                                            vel_noise = LinearNoise(min_noise=0.5, increase_rate=0.05)),
                   ego_goal = LaneTag(2, 1),
                   max_cars=1, 
                   max_peds=1, 
                   car_birth=0.3, 
                   ped_birth=0.3, 
                   obs_dist = ObstacleDistribution(mdp.env, upper_obs_pres_prob=0., left_obs_pres_prob=1.0, right_obs_pres_prob=1.0),
                   max_obstacles=1, # no fixed obstacles
                   lidar=false,
                   ego_start=20,
                   Î”T=0.1)

policy = RandomHoldPolicy(pomdp, 5, 0, UrbanAction(0.), rng);

## Generate data 
folder = "/scratch/boutonm/"
max_steps = 400
n_train = 500
if !isfile(folder*"train_"*string(seed)*".jld2")
    println("Generating $n_train examples of training data")
    train_X, train_Y = collect_set(pomdp, policy, max_steps, rng, n_train)
    save(folder*"train_"*string(seed)*".jld2", "train_X", train_X, "train_Y", train_Y)
else
    println("Loading existing training data: "*"train_"*string(seed)*".jld2")
    train_data = load(folder*"train_"*string(seed)*".jld2")
    train_X, train_Y = train_data["train_X"], train_data["train_Y"]
end
n_val = 100
if !isfile("/scratch/boutonm/val_"*string(seed)*".jld2")
    println("Generating $n_val examples of validation data")
    val_X, val_Y = collect_set(pomdp, policy, max_steps, rng, n_val)
    save(folder*"train_"*string(seed)*".jld2", "val_X", val_X, "val_Y", val_Y)
else
    println("Loading existing validation data: "*"val_"*string(seed)*".jld2")
    val_data = load(folder*"train_"*string(seed)*".jld2")
    val_X, val_Y = val_data["val_X"], val_data["val_Y"]
end

if parsed_args["resume"] == -1
    model_name = "model_"*string(parsed_args["seed"])
else
    model_name = "model_"*string(parsed_args["resume"])
end
if parsed_args["resume"] == -1
    input_length = n_dims(pomdp) 
    n_features = 5
    output_length = n_features*(pomdp.max_cars + pomdp.max_peds)

    model = Chain(LSTM(input_length, 128),
              Dense(128, 64, relu),
              Dense(64, output_length))
else
    println("Loading existing model")
    model = BSON.load(model_name*".bson")[:model]
end

macro interrupts(ex)
  :(try $(esc(ex))
    catch e
      e isa InterruptException || rethrow()
      throw(e)
    end)
end

function loss(x, y)
    mask = build_presence_mask.(y) # same size as y
    l = mean(mse.(model.(x), y, mask)) # mean over the trajectory
    Flux.truncate!(model)
    Flux.reset!(model)
    return l
end

function mse(ypred, y, mask)
    return sum(mask.*(ypred - y).^2)/length(y)
end

function training!(loss, train_data, validation_data, optimizer, n_epochs::Int64; logdir::String="log/model/")
    set_tb_logdir(logdir)
    total_time = 0.
    grad_norm = 0.
    println("Starting training")
    for ep in 1:n_epochs 
        epoch_time = @elapsed begin
            opt = Flux.Optimise.runall(optimizer)
            training_loss = 0.
            for d in train_data 
                l = loss(d...)
                @interrupts Flux.back!(l)
                grad_norm = global_norm(params(model))
                opt()
                training_loss += l.tracker.data
                flush(STDOUT) 
            end
        end
        # log 
        total_time += epoch_time
        training_loss /= n_epochs
        println("eval validation loss")
        validation_loss = 0. 
        for d in validation_data
            val_l = loss(d...)
            validation_loss += val_l.tracker.data
        end
        validation_loss /= length(validation_data)
        set_tb_step!(ep)
        @tb_log training_loss
        set_tb_step!(ep)
        @tb_log validation_loss
        set_tb_step!(ep)
        @tb_log grad_norm
        @save model_name*".bson" model
        logg = @sprintf("%5d / %5d Train loss %0.3e |  Val loss %1.3e | Grad %2.3e | Epoch time (s) %2.1f | Total time (s) %2.1f",
                                ep, n_epochs, training_loss, validation_loss, grad_norm, epoch_time, total_time)
        println(logg)
        flush(STDOUT)
    end 
end

optimizer = ADAM(Flux.params(model), 1e-3)

n_epochs = 10
training!(loss, zip(train_X, train_Y), zip(val_X, val_Y), optimizer, n_epochs, logdir="log/"*model_name)

@save model_name*".bson" model
