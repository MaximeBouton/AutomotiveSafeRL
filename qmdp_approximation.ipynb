{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Belief State Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /mnt/c/Users/Maxime/wsl/.julia/compiled/v1.0/MDPModelChecking/XrC4q.ji for MDPModelChecking [abefb91b-a28c-5ab9-9bd9-026e532d7b0e]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Info: Recompiling stale cache file /mnt/c/Users/Maxime/wsl/.julia/compiled/v1.0/PedCar/NmDDZ.ji for PedCar [90cf7f26-d5c7-593d-a0e1-4a8367407571]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Warning: Package PedCar does not have AutomotivePOMDPs in its dependencies:\n",
      "│ - If you have PedCar checked out for development and have\n",
      "│   added AutomotivePOMDPs as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with PedCar\n",
      "└ Loading AutomotivePOMDPs into PedCar from project dependency, future warnings for PedCar are suppressed.\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "using Printf\n",
    "using StaticArrays\n",
    "using Flux\n",
    "using FileIO\n",
    "using BSON\n",
    "using BSON: @load\n",
    "using ProgressMeter\n",
    "using POMDPs\n",
    "using POMDPModelTools\n",
    "using POMDPSimulators\n",
    "using POMDPPolicies\n",
    "using BeliefUpdaters\n",
    "using DeepRL\n",
    "using DeepQLearning\n",
    "using LocalApproximationValueIteration\n",
    "using DiscreteValueIteration\n",
    "using AutomotiveDrivingModels\n",
    "using AutoViz\n",
    "using AutomotivePOMDPs\n",
    "using MDPModelChecking\n",
    "using PedCar\n",
    "using AutomotiveSensors\n",
    "using Reel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"masking.jl\")\n",
    "include(\"util.jl\")\n",
    "include(\"masked_dqn.jl\")\n",
    "include(\"render_helpers.jl\")\n",
    "include(\"qmdp_approximation.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"training_scripts/RNNFiltering/RNNFiltering.jl\")\n",
    "using Main.RNNFiltering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(1)\n",
    "cam = FitToContentCamera(0.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp = PedCarMDP(pos_res=2.0, vel_res=2., ped_birth=0.7, car_birth=0.7)\n",
    "pomdp = UrbanPOMDP(env=mdp.env,\n",
    "                   sensor = PerfectSensor(),\n",
    "#                     sensor = GaussianSensor(false_positive_rate=0.0, \n",
    "#                                             pos_noise = LinearNoise(min_noise=0.5, increase_rate=0.05), \n",
    "#                                             vel_noise = LinearNoise(min_noise=0.5, increase_rate=0.05)),\n",
    "                   ego_goal = LaneTag(2, 1),\n",
    "                   obs_dist = ObstacleDistribution(mdp.env, \n",
    "                                                   upper_obs_pres_prob=0., \n",
    "                                                   left_obs_pres_prob=1.0, \n",
    "                                                   right_obs_pres_prob=1.0),\n",
    "                   max_cars=1, \n",
    "                   max_peds=1, \n",
    "                   car_birth=0.1, \n",
    "                   ped_birth=0.1, \n",
    "                   max_obstacles=1, # no fixed obstacles\n",
    "                   lidar=false,\n",
    "                   ego_start=20,\n",
    "                   ΔT=0.1);\n",
    "\n",
    "# instantiate sub problems\n",
    "dqn_pomdp = deepcopy(pomdp)\n",
    "dqn_pomdp.max_obstacles = 0\n",
    "\n",
    "rnn_pomdp = deepcopy(pomdp)\n",
    "rnn_pomdp.max_obstacles = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DRQN Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.99\n",
    "@load \"pc_processed.bson\" qmat util pol\n",
    "safe_policy = ValueIterationPolicy(mdp, qmat, util, pol)\n",
    "mask = SafetyMask(mdp, safe_policy, threshold);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-11 09:58:05.754442: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n"
     ]
    }
   ],
   "source": [
    "problem_file=\"training_scripts/drqn-log/log8/problem.bson\"\n",
    "weights_file=\"training_scripts/drqn-log/log8/weights.bson\"\n",
    "env_ = POMDPEnvironment(dqn_pomdp)\n",
    "dqn_policy = DeepQLearning.restore(env_, problem_file=problem_file, weights_file=weights_file)\n",
    "policy = MaskedNNPolicy(dqn_pomdp, dqn_policy, mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RNN Belief Updater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 5\n",
    "models = Vector{Chain}(undef, n_models)\n",
    "for i=1:n_models\n",
    "    models[i] = BSON.load(\"training_scripts/RNNFiltering/model_$(i)0.bson\")[:model] \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"qmdp_approximation.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation with RNN Belief Updater**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp.sensor = GaussianSensor(false_positive_rate=0.0, \n",
    "                            pos_noise = LinearNoise(min_noise=0.5, increase_rate=0.05), \n",
    "                            vel_noise = LinearNoise(min_noise=0.5, increase_rate=0.05));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.096438 seconds (4.52 M allocations: 328.362 MiB, 9.06% gc time)\n"
     ]
    }
   ],
   "source": [
    "up = PedCarRNNUpdater(models, mdp, rnn_pomdp)\n",
    "reset_updater!(up)\n",
    "DeepQLearning.reset_hidden_state!(policy)\n",
    "s0 = initialstate(pomdp, rng)\n",
    "a0 = UrbanAction(1.0)\n",
    "o0 = generate_o(pomdp, s0, a0, s0, rng)\n",
    "b0 = update(up, PedCarRNNBelief(Vector{Vector{Float64}}(undef, n_models), o0), a0, o0);\n",
    "singleaction_policy = FunctionPolicy(s -> UrbanAction(0.))\n",
    "hr = HistoryRecorder(rng=rng, max_steps=400)\n",
    "@time hist = simulate(hr, pomdp, policy, up, b0, s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video autoplay controls><source src=\"files/reel-835309418767342150.webm?8952852967027121121\" type=\"video/webm\"></video>"
      ],
      "text/plain": [
       "Frames{MIME{Symbol(\"image/png\")}}(\"/tmp/tmptIzZdC\", 0x0000000000000041, 10.0, nothing)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the simulation\n",
    "state_history = state_hist(hist)\n",
    "action_history = action_hist(hist)\n",
    "safe_actions_hist = ainfo_hist(hist)\n",
    "observation_history = observation_hist(hist)\n",
    "insert!(observation_history, 1, o0)\n",
    "belief_history = belief_hist(hist)\n",
    "push!(action_history, UrbanAction(NaN))\n",
    "push!(safe_actions_hist, UrbanAction[])\n",
    "duration, fps, render_hist = animate_hist(pomdp, state_history, observation_history, belief_history, action_history, safe_actions_hist)\n",
    "speed_factor = 1\n",
    "film = roll(render_hist, fps = speed_factor*fps, duration = duration/speed_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluation_loop (generic function with 3 methods)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluation_loop(pomdp::UrbanPOMDP, policy::Policy, up::PedCarRNNUpdater; n_ep::Int64 = 1000, max_steps::Int64 = 500, rng::AbstractRNG = Base.GLOBAL_RNG)\n",
    "    rewards = zeros(n_ep)\n",
    "    steps = zeros(n_ep)\n",
    "    violations = zeros(n_ep)\n",
    "    @showprogress for ep=1:n_ep\n",
    "        reset_updater!(up)\n",
    "        DeepQLearning.reset_hidden_state!(policy)\n",
    "        s0 = initialstate(pomdp, rng)\n",
    "        a0 = UrbanAction(1.0)\n",
    "        o0 = generate_o(pomdp, s0, a0, s0, rng)\n",
    "        b0 = update(up, PedCarRNNBelief(Vector{Vector{Float64}}(undef, n_models), o0), a0, o0);\n",
    "        singleaction_policy = FunctionPolicy(s -> UrbanAction(0.))\n",
    "        hr = HistoryRecorder(rng=rng, max_steps=400)\n",
    "        hist = simulate(hr, pomdp, policy, up, b0, s0);\n",
    "        rewards[ep] = discounted_reward(hist)\n",
    "        steps[ep] = n_steps(hist)\n",
    "        violations[ep] = is_crash(hist.state_hist[end])#sum(hist.reward_hist .<= -1.) #+ Int(n_steps(hist) >= max_steps)\n",
    "    end\n",
    "    return rewards, steps, violations\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████|  ETA: 0:00:01\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046.236362 seconds (2.40 G allocations: 172.741 GiB, 12.58% gc time)\n",
      "Summary for "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:17:26\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 episodes: \n",
      "Average reward: 0.018 \n",
      "Average # of steps: 86.592 \n",
      "Average # of violations: 7.800 \n"
     ]
    }
   ],
   "source": [
    "up = PedCarRNNUpdater(models, mdp, rnn_pomdp)\n",
    "@time rewards_mask, steps_mask, violations_mask = evaluation_loop(pomdp, policy, up, n_ep=1000, max_steps=400, rng=rng);\n",
    "print_summary(rewards_mask, steps_mask, violations_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation with perfect observation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerfectSensor()"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp.sensor = PerfectSensor();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.191908 seconds (857.43 k allocations: 33.055 MiB, 14.02% gc time)\n"
     ]
    }
   ],
   "source": [
    "up = PerfectSensorUpdater(dqn_pomdp)\n",
    "DeepQLearning.reset_hidden_state!(policy)\n",
    "s0 = initialstate(pomdp, rng)\n",
    "a0 = UrbanAction(1.0)\n",
    "o0 = generate_o(pomdp, s0, a0, s0, rng)\n",
    "b0 = update(up, o0, a0, o0)\n",
    "singleaction_policy = FunctionPolicy(s -> UrbanAction(0.))\n",
    "hr = HistoryRecorder(rng=rng, max_steps=400)\n",
    "@time hist = simulate(hr, pomdp, policy, up, b0, s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video autoplay controls><source src=\"files/reel-3937198213732385379.webm?4487677827039628990\" type=\"video/webm\"></video>"
      ],
      "text/plain": [
       "Frames{MIME{Symbol(\"image/png\")}}(\"/tmp/tmpd8H2Ek\", 0x0000000000000041, 10.0, nothing)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_history = state_hist(hist)\n",
    "action_history = action_hist(hist)\n",
    "safe_actions_hist = ainfo_hist(hist)\n",
    "observation_history = observation_hist(hist)\n",
    "insert!(observation_history, 1, o0)\n",
    "belief_history = belief_hist(hist)\n",
    "push!(action_history, UrbanAction(NaN))\n",
    "push!(safe_actions_hist, UrbanAction[])\n",
    "duration, fps, render_hist = animate_hist(pomdp, state_history, observation_history, action_history, safe_actions_hist)\n",
    "speed_factor = 1\n",
    "film = roll(render_hist, fps = speed_factor*fps, duration = duration/speed_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulation with multiple cars and pedestrians**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.868439 seconds (3.82 M allocations: 112.693 MiB, 5.61% gc time)\n"
     ]
    }
   ],
   "source": [
    "pomdp.max_cars = 5\n",
    "pomdp.max_peds = 5\n",
    "still_policy = FunctionPolicy(s -> UrbanAction(0.))\n",
    "up = NothingUpdater()\n",
    "s0 = initialstate(pomdp, rng)\n",
    "hr = HistoryRecorder(rng=rng, max_steps=400)\n",
    "@time hist = simulate(hr, pomdp, still_policy, up, b0, s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video autoplay controls><source src=\"files/reel-1070636769396691234.webm?15772733053729999738\" type=\"video/webm\"></video>"
      ],
      "text/plain": [
       "Frames{MIME{Symbol(\"image/png\")}}(\"/tmp/tmp1deaqo\", 0x0000000000000191, 10.0, nothing)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration, fps, render_rec = animate_scenes(hist.state_hist, pomdp.env, sim_dt=pomdp.ΔT, cam = StaticCamera(VecE2(0., -8.), 14.0))\n",
    "speed_factor = 1\n",
    "film = roll(render_rec, fps = speed_factor*fps, duration = duration/speed_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(hist.state_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.9999913828130208\n",
       " 0.9999913828130208\n",
       " 0.9999913828130208\n",
       " 0.9922843380022387"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = state_history[1]\n",
    "o = observation_history[1]\n",
    "p_sa = compute_probas(pomdp, policy.mask, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "function AutomotivePOMDPs.animate_hist(pomdp::UrbanPOMDP, \n",
    "                                         scenes::Vector{Scene}, \n",
    "                                         observations::Vector{Vector{Float64}}, \n",
    "                                         beliefs::Vector{PedCarRNNBelief}, \n",
    "                                         actions::Vector{UrbanAction},\n",
    "                                         safe_actions::Vector{Any};\n",
    "                                         sim_dt = 0.1,\n",
    "                                         cam = StaticCamera(VecE2(0., -8.), 14.0))\n",
    "    env = pomdp.env \n",
    "    duration = length(scenes)*sim_dt\n",
    "    fps = Int(1/sim_dt)\n",
    "    function render_rec(t, dt)\n",
    "        frame_index = Int(floor(t/dt)) + 1\n",
    "        overlays = SceneOverlay[IDOverlay()]\n",
    "        obs = [veh for veh in obs_to_scene(pomdp, observations[frame_index]) if veh.id != EGO_ID]\n",
    "        obs_overlay = GaussianSensorOverlay(sensor=pomdp.sensor, o=obs, color=MONOKAI[\"color2\"])\n",
    "        push!(overlays, obs_overlay)\n",
    "        occlusion_overlay = OcclusionOverlay(obstacles=mdp.env.obstacles)\n",
    "        push!(overlays, occlusion_overlay)\n",
    "        cp, pp = 0., 0.\n",
    "        for pred in beliefs[frame_index].predictions\n",
    "            bb, car_pres, ped_pres = process_prediction(pomdp, pred, beliefs[frame_index].obs)\n",
    "            cp += car_pres\n",
    "            pp += ped_pres\n",
    "            bel = [veh for veh in obs_to_scene(pomdp, bb) if veh.id != EGO_ID]\n",
    "            bel_overlay = GaussianSensorOverlay(sensor=pomdp.sensor, o=bel, color=MONOKAI[\"color4\"])            \n",
    "            push!(overlays, bel_overlay)\n",
    "        end\n",
    "        cp /= length(beliefs[frame_index].predictions)\n",
    "        pp /= length(beliefs[frame_index].predictions)\n",
    "        push!(overlays, HistogramOverlay(pos=VecE2(-15., -20.), val=cp, label=\"car\"))\n",
    "        push!(overlays, HistogramOverlay(pos=VecE2(-12., -20.), val=pp, label=\"ped\"))\n",
    "        push!(overlays, TextOverlay(text=[\"Probability of presence\"], pos=VecSE2(-17,-14.), font_size=15, incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"v: $(get_ego(scenes[frame_index]).state.v)\"], font_size=20, \n",
    "                                    pos=VecE2(pomdp.env.params.x_min + 3.,6.), incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"Acc: $(actions[frame_index].acc)\"], font_size=20,\n",
    "                                    pos=VecE2(pomdp.env.params.x_min + 3.,8.), incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"Available Actions: $([a.acc for a in safe_actions[frame_index]])\"], font_size=20,\n",
    "                                    pos=VecE2(pomdp.env.params.x_min + 3.,10.), incameraframe=true))\n",
    "        push!(overlays, TextOverlay(text = [\"step: $frame_index\"], font_size=20,\n",
    "                                            pos=VecE2(pomdp.env.params.x_min + 3.,4.), incameraframe=true))\n",
    "                                \n",
    "        return AutoViz.render(scenes[frame_index], env, overlays, cam=cam)\n",
    "    end\n",
    "    return duration, fps, render_rec\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
